{"name":"Reddit Image Scraper","tagline":"Scanning imgur for submissions of reddit users","body":"# Reddit-Image-Scraper\r\n\r\n## Purpose\r\nThis python script enables you to pick a reddit user and automatically download all pictures he / she posted in Imgur links.\r\nThe number of posts which are scanned can be specified by a number.\r\n\r\n## Setup\r\n\r\n### Python\r\n\r\n[Python 2](https://www.python.org/download/releases/2.7/) or [Python 3](https://www.python.org/download/releases/3.4.1/) must be installed on your computer.\r\n\r\n### Dependencies\r\n\r\nTo use this script, you will need the libraries [praw](https://github.com/praw-dev/praw) and [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/).\r\nThey can be installed by the `pip` command through your command line as follows:\r\n\r\n```\r\n$ pip install -r requirements.txt\r\n```\r\n\r\nAfter that, you're good to go!\r\n\r\n### Installation\r\n\r\nTo install simply clone the github repo and run setup.py\r\n\r\n```\r\n# Clone the Repo\r\n$ git clone https://github.com/Rookev/Reddit-Image-Scraper.git\r\n\r\n# Install\r\n$ python setup.py install\r\n```\r\n\r\n## Usage\r\n\r\nAfter installing the package, the console script `reddit_scraper`\r\ncan now be used.\r\n\r\n### Example\r\n\r\nChoose a reddit user and download each of his or her Imgur submissions to your local hard drive\r\n\r\n```\r\n$ reddit_scraper User123 5\r\n```\r\n\r\nThis command scrapes the last 5 posts of `/u/User123`, detects his Imgur submissions and downloads it to your desktop in a newly created folder called `/User123/`\r\n\r\n## Documentation\r\n\r\nDocumentation available at [readthedocs](http://reddit-image-scraper.readthedocs.org/)\r\n\r\n## Contributions\r\n\r\nA more pythonized programming style and documentation with Sphinx was contributed by [Sang Han](https://github.com/jjangsangy)\r\n\r\n## Personal annotation\r\nWith this script I wanted to try out some techniques and skills, so it also had some personal goals:\r\n\r\n\r\n* Python (development & publishing / installing)\r\n* Web scraping\r\n* Using the social aspect of GitHub (pull requests, merging)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}